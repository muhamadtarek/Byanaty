{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Set paths\n",
    "ids_dir = r'C:\\Users\\Mohamed\\Desktop\\projects\\Byanaty\\backend\\base\\dataset'\n",
    "test_dir = r'C:\\Users\\Mohamed\\Desktop\\projects\\Byanaty\\backend\\base\\dataset\\test'\n",
    "\n",
    "# Function to perform train-test split and move images\n",
    "def split_data_and_move(ids_dir, test_dir, test_size=0.2, random_state=42):\n",
    "    # List all real and fake ID images\n",
    "    real_images = os.listdir(os.path.join(ids_dir, 'real'))\n",
    "    fake_images = os.listdir(os.path.join(ids_dir, 'fake'))\n",
    "    \n",
    "    # Split real and fake images into train and test sets\n",
    "    train_real, test_real = train_test_split(real_images, test_size=test_size, random_state=random_state)\n",
    "    train_fake, test_fake = train_test_split(fake_images, test_size=test_size, random_state=random_state)\n",
    "    \n",
    "    # Create test directory if it doesn't exist\n",
    "    if not os.path.exists(test_dir):\n",
    "        os.makedirs(test_dir)\n",
    "    \n",
    "    # Create real and fake directories in test directory if they don't exist\n",
    "    if not os.path.exists(os.path.join(test_dir, 'real')):\n",
    "        os.makedirs(os.path.join(test_dir, 'real'))\n",
    "    if not os.path.exists(os.path.join(test_dir, 'fake')):\n",
    "        os.makedirs(os.path.join(test_dir, 'fake'))\n",
    "    \n",
    "    # Move real images to test directory\n",
    "    for image in test_real:\n",
    "        src = os.path.join(ids_dir, 'real', image)\n",
    "        dst = os.path.join(test_dir, 'real', image)\n",
    "        shutil.move(src, dst)\n",
    "        \n",
    "    # Move fake images to test directory\n",
    "    for image in test_fake:\n",
    "        src = os.path.join(ids_dir, 'fake', image)\n",
    "        dst = os.path.join(test_dir, 'fake', image)\n",
    "        shutil.move(src, dst)\n",
    "    \n",
    "    print(f\"Train-Test split completed. Test images moved to {test_dir}\")\n",
    "\n",
    "# Perform train-test split\n",
    "split_data_and_move(ids_dir, test_dir, test_size=0.2, random_state=42)\n",
    "\n",
    "# Set the new directory paths after split\n",
    "train_dir = ids_dir  # Training data remains in original directory\n",
    "val_dir = test_dir  # Validation data moved to test directory\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Function to calculate average dimensions of real ID images\n",
    "def calculate_average_dimensions(directory):\n",
    "    total_width = 0\n",
    "    total_height = 0\n",
    "    count = 0\n",
    "\n",
    "    for filename in os.listdir(os.path.join(directory, 'real')):\n",
    "        if filename.endswith(('.jpg', '.jpeg', '.png')):\n",
    "            img_path = os.path.join(directory, 'real', filename)\n",
    "            img = tf.keras.preprocessing.image.load_img(img_path)\n",
    "            width, height = img.size\n",
    "            total_width += width\n",
    "            total_height += height\n",
    "            count += 1\n",
    "    \n",
    "    if count > 0:\n",
    "        avg_width = total_width // count\n",
    "        avg_height = total_height // count\n",
    "    else:\n",
    "        avg_width, avg_height = 0, 0\n",
    "\n",
    "    return avg_width, avg_height\n",
    "\n",
    "# Calculate average dimensions for the resized images\n",
    "IMG_WIDTH, IMG_HEIGHT = calculate_average_dimensions(train_dir)\n",
    "\n",
    "# Generator function for GAN\n",
    "def build_generator(latent_dim, img_shape):\n",
    "    model = models.Sequential()\n",
    "    model.add(layers.Dense(128 * 16 * 16, activation='relu', input_dim=latent_dim))\n",
    "    model.add(layers.Reshape((16, 16, 128)))\n",
    "    model.add(layers.Conv2DTranspose(128, kernel_size=4, strides=2, padding='same', activation='relu'))\n",
    "    model.add(layers.Conv2DTranspose(128, kernel_size=4, strides=2, padding='same', activation='relu'))\n",
    "    model.add(layers.Conv2DTranspose(64, kernel_size=4, strides=2, padding='same', activation='relu'))\n",
    "    model.add(layers.Conv2D(3, kernel_size=3, padding='same', activation='sigmoid'))\n",
    "    model.summary()\n",
    "    return model\n",
    "\n",
    "# Discriminator function for GAN\n",
    "def build_discriminator(img_shape):\n",
    "    model = models.Sequential()\n",
    "    model.add(layers.Conv2D(64, kernel_size=3, strides=2, input_shape=img_shape, padding='same'))\n",
    "    model.add(layers.LeakyReLU(alpha=0.2))\n",
    "    model.add(layers.Dropout(0.4))\n",
    "    model.add(layers.Conv2D(128, kernel_size=3, strides=2, padding='same'))\n",
    "    model.add(layers.LeakyReLU(alpha=0.2))\n",
    "    model.add(layers.Dropout(0.4))\n",
    "    model.add(layers.Flatten())\n",
    "    model.add(layers.Dense(1, activation='sigmoid'))\n",
    "    model.summary()\n",
    "    return model\n",
    "\n",
    "# Combined GAN model\n",
    "def build_gan(generator, discriminator):\n",
    "    discriminator.trainable = False\n",
    "    model = models.Sequential()\n",
    "    model.add(generator)\n",
    "    model.add(discriminator)\n",
    "    return model\n",
    "\n",
    "# Build the generator\n",
    "generator = build_generator(latent_dim=100, img_shape=(IMG_HEIGHT, IMG_WIDTH, 3))\n",
    "\n",
    "# Build the discriminator\n",
    "discriminator = build_discriminator(img_shape=(IMG_HEIGHT, IMG_WIDTH, 3))\n",
    "\n",
    "# Build the GAN\n",
    "gan = build_gan(generator, discriminator)\n",
    "\n",
    "# Compile the discriminator (only during training)\n",
    "discriminator.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Compile the GAN (stacked generator and discriminator, only during training)\n",
    "gan.compile(optimizer='adam', loss='binary_crossentropy')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data generators with augmentation for training and validation\n",
    "train_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "# Generate batches of real ID images\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    train_dir,\n",
    "    target_size=(IMG_HEIGHT, IMG_WIDTH),\n",
    "    batch_size=16,\n",
    "    class_mode=None  # Generator will produce images, not classify them\n",
    ")\n",
    "\n",
    "# Train the GAN\n",
    "epochs = 10\n",
    "fixed_noise = np.random.normal(0, 1, (16, 100))\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    for batch in train_generator:\n",
    "        # Generate random noise as input to the generator\n",
    "        noise = np.random.normal(0, 1, (16, 100))  # Batch size = 16, latent dim = 100\n",
    "        \n",
    "        # Generate fake images using the generator\n",
    "        gen_images = generator.predict(noise)\n",
    "        \n",
    "        # Train the discriminator (real classified as ones and generated as zeros)\n",
    "        d_loss_real = discriminator.train_on_batch(batch, np.ones((16, 1)))\n",
    "        d_loss_fake = discriminator.train_on_batch(gen_images, np.zeros((16, 1)))\n",
    "        \n",
    "        # Train the generator (attempt to fool discriminator by classifying as real)\n",
    "        g_loss = gan.train_on_batch(noise, np.ones((16, 1)))\n",
    "        \n",
    "        # Print progress\n",
    "        print(f'Epoch: {epoch + 1}, [D loss real: {d_loss_real[0]}, D loss fake: {d_loss_fake[0]}, G loss: {g_loss}]')\n",
    "        \n",
    "        break  # Only train on one batch for simplicity, remove this break for full training\n",
    "\n",
    "    # Optionally, save generated images\n",
    "    if (epoch + 1) % 5 == 0:\n",
    "        # Generate images from fixed noise for visualization\n",
    "        gen_imgs = generator.predict(fixed_noise)\n",
    "        # Save images or any other visualization steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the discriminator on the test set for anomaly detection\n",
    "def detect_anomalies(test_generator, discriminator):\n",
    "    anomalies = []\n",
    "    for batch in test_generator:\n",
    "        # Predict using the discriminator\n",
    "        predictions = discriminator.predict(batch)\n",
    "        anomalies.extend(predictions)\n",
    "    \n",
    "    return anomalies\n",
    "\n",
    "# Evaluate anomalies on the test set\n",
    "test_generator = train_datagen.flow_from_directory(\n",
    "    test_dir,\n",
    "    target_size=(IMG_HEIGHT, IMG_WIDTH),\n",
    "    batch_size=1,  # Evaluate one image at a time\n",
    "    class_mode=None,  # No class labels needed for anomaly detection\n",
    "    shuffle=False  # Ensure data is not shuffled\n",
    ")\n",
    "\n",
    "anomalies = detect_anomalies(test_generator, discriminator)\n",
    "print(anomalies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "445 527\n"
     ]
    }
   ],
   "source": [
    "print(IMG_HEIGHT, IMG_WIDTH)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "backend-twFdXziY",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
